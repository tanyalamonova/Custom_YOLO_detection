{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlhBExfZbIdF",
        "outputId": "524fde72-ee13-42d8-80b2-ca7a937bed65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FivKzIu4bIdG",
        "outputId": "a20c4084-45e5-4533-8d08-b0b88beea845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/pets/YOLO_test\n"
          ]
        }
      ],
      "source": [
        "%cd gdrive/'My Drive'/pets/YOLO_test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOW9dmLjcfnQ",
        "outputId": "82fc0fd6-b728-4ff4-9702-a6d9e8ff3cc5"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/WongKinYiu/yolov7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2Kb-0C1XqBy"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -r yolov7/requirements.txt\n",
        "pip install fiftyone\n",
        "pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data analysis using fiftyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNxy61XYjPnA",
        "outputId": "441c44df-a890-4e71-d158-afa19a6b6134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Migrating database to v0.17.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.migrations.runner:Migrating database to v0.17.2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2577/2577 [13.7m elapsed, 0s remaining, 5.0 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2577/2577 [13.7m elapsed, 0s remaining, 5.0 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645/645 [3.4m elapsed, 0s remaining, 3.8 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645/645 [3.4m elapsed, 0s remaining, 3.8 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name:        yolo-dataset\n",
            "Media type:  image\n",
            "Num samples: 3222\n",
            "Persistent:  False\n",
            "Tags:        []\n",
            "Sample fields:\n",
            "    id:           fiftyone.core.fields.ObjectIdField\n",
            "    filepath:     fiftyone.core.fields.StringField\n",
            "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
            "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
            "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
            "[<Sample: {\n",
            "    'id': '63561a2063b8523b3d4c75e1',\n",
            "    'media_type': 'image',\n",
            "    'filepath': '/content/gdrive/My Drive/pets/YOLO_test/train/images/00000001.jpg',\n",
            "    'tags': ['train'],\n",
            "    'metadata': None,\n",
            "    'ground_truth': <Detections: {'detections': []}>,\n",
            "}>, <Sample: {\n",
            "    'id': '63561a2163b8523b3d4c75eb',\n",
            "    'media_type': 'image',\n",
            "    'filepath': '/content/gdrive/My Drive/pets/YOLO_test/train/images/00000002.jpg',\n",
            "    'tags': ['train'],\n",
            "    'metadata': None,\n",
            "    'ground_truth': <Detections: {\n",
            "        'detections': [\n",
            "            <Detection: {\n",
            "                'id': '63561a2063b8523b3d4c75e2',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'uniform',\n",
            "                'bounding_box': [0.73125, 0.5375, 0.01875, 0.027777999999999997],\n",
            "                'mask': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "            }>,\n",
            "            <Detection: {\n",
            "                'id': '63561a2063b8523b3d4c75e3',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'uniform',\n",
            "                'bounding_box': [0.7234379999999999, 0.3625, 0.0125, 0.034722],\n",
            "                'mask': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "            }>,\n",
            "            <Detection: {\n",
            "                'id': '63561a2063b8523b3d4c75e4',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'head',\n",
            "                'bounding_box': [0.729688, 0.3499995, 0.007812, 0.016667],\n",
            "                'mask': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "            }>,\n",
            "            <Detection: {\n",
            "                'id': '63561a2063b8523b3d4c75e5',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'body',\n",
            "                'bounding_box': [0.7203125, 0.34861149999999996, 0.019531, 0.076389],\n",
            "                'mask': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "            }>,\n",
            "            <Detection: {\n",
            "                'id': '63561a2063b8523b3d4c75e6',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'body',\n",
            "                'bounding_box': [\n",
            "                    0.7250004999999998,\n",
            "                    0.5333334999999999,\n",
            "                    0.029687000000000005,\n",
            "                    0.083333,\n",
            "                ],\n",
            "                'mask': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "            }>,\n",
            "        ],\n",
            "    }>,\n",
            "}>, <Sample: {\n",
            "    'id': '63561a2163b8523b3d4c75ec',\n",
            "    'media_type': 'image',\n",
            "    'filepath': '/content/gdrive/My Drive/pets/YOLO_test/train/images/00000003.jpg',\n",
            "    'tags': ['train'],\n",
            "    'metadata': None,\n",
            "    'ground_truth': <Detections: {\n",
            "        'detections': [\n",
            "            <Detection: {\n",
            "                'id': '63561a2163b8523b3d4c75e7',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'uniform',\n",
            "                'bounding_box': [0.1507815, 0.388889, 0.015625, 0.034722],\n",
            "                'mask': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "            }>,\n",
            "            <Detection: {\n",
            "                'id': '63561a2163b8523b3d4c75e8',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'uniform',\n",
            "                'bounding_box': [0.854687, 0.365278, 0.017187999999999995, 0.034722],\n",
            "                'mask': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "            }>,\n",
            "            <Detection: {\n",
            "                'id': '63561a2163b8523b3d4c75e9',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'body',\n",
            "                'bounding_box': [\n",
            "                    0.8515619999999998,\n",
            "                    0.3597225,\n",
            "                    0.027344,\n",
            "                    0.06388900000000002,\n",
            "                ],\n",
            "                'mask': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "            }>,\n",
            "            <Detection: {\n",
            "                'id': '63561a2163b8523b3d4c75ea',\n",
            "                'attributes': {},\n",
            "                'tags': [],\n",
            "                'label': 'body',\n",
            "                'bounding_box': [0.145313, 0.381944, 0.028905999999999998, 0.052778],\n",
            "                'mask': None,\n",
            "                'confidence': None,\n",
            "                'index': None,\n",
            "            }>,\n",
            "        ],\n",
            "    }>,\n",
            "}>]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import fiftyone as fo\n",
        "\n",
        "name = \"yolo-dataset\"\n",
        "\n",
        "try:\n",
        "    dataset = fo.Dataset(name)\n",
        "\n",
        "    dataset_dir = os.getcwd()\n",
        "    splits = [\"train\", \"val\"]\n",
        "\n",
        "    for split in splits:\n",
        "        dataset.add_dir(\n",
        "            dataset_dir=dataset_dir,\n",
        "            dataset_type=fo.types.YOLOv5Dataset,\n",
        "            split=split,\n",
        "            tags=split,\n",
        "    )\n",
        "except Exception:\n",
        "    print('This dataset already exists')\n",
        "    fo.load_dataset(name)\n",
        "\n",
        "print(dataset)\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "id": "Z0ivyxZeHBjm",
        "outputId": "7ce3046d-f9d6-4740-e8b4-2cb96e172a50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "#focontainer-002f2558-7449-4c82-9649-56965a080d3c {\n",
              "  position: relative;\n",
              "  height: px;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-002f2558-7449-4c82-9649-56965a080d3c {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-002f2558-7449-4c82-9649-56965a080d3c:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-002f2558-7449-4c82-9649-56965a080d3c {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-002f2558-7449-4c82-9649-56965a080d3c\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-002f2558-7449-4c82-9649-56965a080d3c\">\n",
              "      <button id=\"foactivate-002f2558-7449-4c82-9649-56965a080d3c\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset:          yolo-dataset\n",
              "Media type:       image\n",
              "Num samples:      3222\n",
              "Selected samples: 0\n",
              "Selected labels:  0\n",
              "Session type:     colab"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7eBrF8nK4k5",
        "outputId": "17db5be9-871f-4c3f-9cab-9eac63d2a490"
      },
      "outputs": [],
      "source": [
        "%cd yolov7/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsURJrhjJm__",
        "outputId": "4f876c48-d544-4dbf-f104-9b13122a9385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLOR ðŸš€ 2022-10-21 torch 1.12.1+cu113 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "Namespace(adam=False, artifact_alias='latest', batch_size=4, bbox_interval=-1, bucket='', cache_images=False, cfg='cfg/training/yolov7.yaml', data='data/custom.yaml', device='', entity=None, epochs=50, evolve=False, exist_ok=False, freeze=[0], global_rank=-1, hyp='data/hyp.scratch.p5.yaml', image_weights=False, img_size=[416, 416], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='yolov7', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/yolov73', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=4, upload_dataset=False, v5_metric=False, weights='weights/yolov7_training.pt', workers=4, world_size=1)\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2hijoe8n.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 12                -1  1         0  models.common.MP                        []                            \n",
            " 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n",
            " 25                -1  1         0  models.common.MP                        []                            \n",
            " 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 38                -1  1         0  models.common.MP                        []                            \n",
            " 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n",
            " 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n",
            " 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 76                -1  1         0  models.common.MP                        []                            \n",
            " 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n",
            " 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 89                -1  1         0  models.common.MP                        []                            \n",
            " 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n",
            " 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n",
            " 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            "100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            "101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n",
            "102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n",
            "103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n",
            "104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n",
            "105   [102, 103, 104]  1     44944  models.yolo.IDetect                     [3, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 415 layers, 37207344 parameters, 37207344 gradients, 105.1 GFLOPS\n",
            "\n",
            "Transferred 555/566 items from weights/yolov7_training.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/gdrive/MyDrive/pets/YOLO_test/train/labels' images and labels... 2468 found, 0 missing, 122 empty, 0 corrupted:  96% 2468/2577 [01:10<00:00, 203.83it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/gdrive/MyDrive/pets/YOLO_test/train/images/8_0000015.jpg: cannot identify image file '/content/gdrive/MyDrive/pets/YOLO_test/train/images/8_0000015.jpg'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/gdrive/MyDrive/pets/YOLO_test/train/labels' images and labels... 2576 found, 0 missing, 128 empty, 1 corrupted: 100% 2577/2577 [01:11<00:00, 36.14it/s] \n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/gdrive/MyDrive/pets/YOLO_test/train/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/gdrive/MyDrive/pets/YOLO_test/val/labels' images and labels... 645 found, 0 missing, 30 empty, 0 corrupted: 100% 645/645 [00:13<00:00, 46.92it/s] \n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/gdrive/MyDrive/pets/YOLO_test/val/labels.cache\n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.41, Best Possible Recall (BPR) = 0.9581. Attempting to improve anchors, please wait...\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 203 of 17628 labels are < 3 pixels in size.\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 17627 points...\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9999 best possible recall, 7.33 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=416, metric_all=0.483/0.853-mean/best, past_thr=0.551-mean: 5,5,  6,6,  8,11,  10,13,  14,16,  11,24,  17,22,  16,31,  28,31\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8577: 100% 1000/1000 [00:04<00:00, 215.46it/s]\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9999 best possible recall, 7.40 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=416, metric_all=0.490/0.858-mean/best, past_thr=0.555-mean: 5,4,  6,6,  8,10,  9,13,  13,14,  11,24,  16,20,  15,29,  25,30\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
            "\n",
            "Image sizes 416 train, 416 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/yolov73\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      0/49     5.96G   0.06491  0.008854   0.01179   0.08555        36       416: 100% 644/644 [10:19<00:00,  1.04it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:34<00:00,  2.32it/s]\n",
            "                 all         645        4721       0.598       0.235       0.191      0.0477\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      1/49     5.92G   0.04458  0.007076  0.005873   0.05753        45       416: 100% 644/644 [10:12<00:00,  1.05it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:32<00:00,  2.49it/s]\n",
            "                 all         645        4721        0.29       0.103      0.0795      0.0201\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      2/49     5.92G   0.05601  0.007348  0.007758   0.07111        21       416: 100% 644/644 [09:54<00:00,  1.08it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:31<00:00,  2.56it/s]\n",
            "                 all         645        4721       0.188       0.121      0.0614      0.0154\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      3/49     5.92G   0.03784  0.006954  0.004004    0.0488        44       416: 100% 644/644 [09:57<00:00,  1.08it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:32<00:00,  2.47it/s]\n",
            "                 all         645        4721       0.232       0.357       0.223      0.0599\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      4/49     5.92G   0.03405  0.007141  0.003019   0.04421        47       416: 100% 644/644 [09:58<00:00,  1.08it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:32<00:00,  2.52it/s]\n",
            "                 all         645        4721       0.314       0.449       0.256      0.0803\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      5/49     5.92G   0.03366  0.007399  0.002632    0.0437        13       416: 100% 644/644 [09:51<00:00,  1.09it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:34<00:00,  2.35it/s]\n",
            "                 all         645        4721       0.477       0.405       0.348         0.1\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      6/49     5.92G   0.03589  0.007273  0.002473   0.04564        44       416: 100% 644/644 [09:48<00:00,  1.10it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:32<00:00,  2.47it/s]\n",
            "                 all         645        4721       0.654       0.424       0.473       0.164\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      7/49     5.92G   0.03807  0.007501  0.002087   0.04766        30       416: 100% 644/644 [09:59<00:00,  1.07it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:33<00:00,  2.44it/s]\n",
            "                 all         645        4721        0.48       0.553       0.496       0.168\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      8/49     5.92G   0.03643  0.007239  0.002056   0.04572        37       416: 100% 644/644 [09:47<00:00,  1.10it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:31<00:00,  2.57it/s]\n",
            "                 all         645        4721        0.61       0.542       0.525        0.18\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      9/49     5.92G   0.03723  0.007394  0.002143   0.04676        40       416: 100% 644/644 [09:48<00:00,  1.09it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:32<00:00,  2.49it/s]\n",
            "                 all         645        4721       0.626       0.552       0.576       0.217\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     10/49     5.92G   0.03906  0.007287   0.00261   0.04896        29       416: 100% 644/644 [09:31<00:00,  1.13it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:38<00:00,  2.11it/s]\n",
            "                 all         645        4721       0.567       0.526       0.509       0.163\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     11/49     5.92G   0.03802  0.007265   0.00212   0.04741       108       416: 100% 644/644 [09:28<00:00,  1.13it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:31<00:00,  2.59it/s]\n",
            "                 all         645        4721       0.646        0.59       0.604       0.222\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     12/49     5.92G   0.03808  0.007086  0.002151   0.04732        49       416: 100% 644/644 [09:44<00:00,  1.10it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:31<00:00,  2.59it/s]\n",
            "                 all         645        4721       0.669       0.553       0.596       0.205\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     13/49     5.92G   0.03821  0.007152  0.001999   0.04736        29       416: 100% 644/644 [09:40<00:00,  1.11it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:32<00:00,  2.52it/s]\n",
            "                 all         645        4721       0.701       0.586       0.628       0.242\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     14/49     5.92G   0.03743  0.007041  0.001778   0.04625        26       416: 100% 644/644 [09:45<00:00,  1.10it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:38<00:00,  2.12it/s]\n",
            "                 all         645        4721       0.733       0.652       0.688       0.265\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     15/49     5.92G   0.03605  0.007326  0.001799   0.04518        34       416: 100% 644/644 [09:42<00:00,  1.11it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:32<00:00,  2.48it/s]\n",
            "                 all         645        4721       0.758       0.662       0.694        0.27\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     16/49     5.92G   0.03541  0.007048  0.001438    0.0439        20       416: 100% 644/644 [09:26<00:00,  1.14it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:31<00:00,  2.59it/s]\n",
            "                 all         645        4721       0.773       0.652       0.703       0.292\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     17/49     5.92G   0.03453  0.007095  0.001205   0.04283         5       416: 100% 644/644 [09:37<00:00,  1.11it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:39<00:00,  2.06it/s]\n",
            "                 all         645        4721       0.753       0.709       0.732       0.307\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     18/49     5.92G   0.03403  0.007011  0.001255   0.04229        12       416: 100% 644/644 [09:33<00:00,  1.12it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 81/81 [00:31<00:00,  2.56it/s]\n",
            "                 all         645        4721       0.839       0.694       0.771       0.333\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     19/49     5.92G   0.03452  0.007295  0.001218   0.04303        49       416:  65% 418/644 [06:08<03:12,  1.18it/s]"
          ]
        }
      ],
      "source": [
        "!python3 train.py --weights weights/yolov7_training.pt --data data/custom.yaml --workers 4 --batch-size 4 --img 416 --cfg cfg/training/yolov7.yaml --name yolov7 --hyp data/hyp.scratch.p5.yaml --epochs 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see all the charts and metrics in readme.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQUfobRmOi5W",
        "outputId": "460f5191-5320-4731-b10c-de0e3c8b7d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(augment=False, batch_size=32, conf_thres=0.001, data='data/custom.yaml', device='0', exist_ok=False, img_size=416, iou_thres=0.65, name='yolov7_416_val', no_trace=False, project='runs/test', save_conf=False, save_hybrid=False, save_json=False, save_txt=False, single_cls=False, task='val', v5_metric=False, verbose=False, weights=['runs/train/yolov73/weights/best.pt'])\n",
            "YOLOR ðŸš€ 2022-10-21 torch 1.12.1+cu113 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 314 layers, 36492560 parameters, 6194944 gradients, 103.2 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/gdrive/MyDrive/pets/YOLO_test/val/labels.cache' images and labels... 645 found, 0 missing, 30 empty, 0 corrupted: 100% 645/645 [00:00<?, ?it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 21/21 [00:43<00:00,  2.07s/it]\n",
            "                 all         645        4721       0.808       0.695       0.763        0.33\n",
            "             uniform         645        1653       0.819       0.753       0.828       0.404\n",
            "                head         645        1402       0.743       0.568       0.603       0.225\n",
            "                body         645        1666       0.863       0.765       0.856        0.36\n",
            "Speed: 4.3/1.7/5.9 ms inference/NMS/total per 416x416 image at batch-size 32\n",
            "Results saved to runs/test/yolov7_416_val\n"
          ]
        }
      ],
      "source": [
        "!python3 test.py --data data/custom.yaml --img 416 --batch 32 --conf 0.001 --iou 0.65 --device 0 --weights runs/train/yolov73/weights/best.pt --name yolov7_416_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing object detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see the results in readme.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-gJANKtSc8L",
        "outputId": "7825f7d0-20e7-4cc8-e4d2-2da0af4c8ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.25, device='', exist_ok=False, img_size=416, iou_thres=0.45, name='exp', no_trace=False, nosave=False, project='runs/detect', save_conf=False, save_txt=False, source='../val/images/1192.jpg', update=False, view_img=False, weights=['runs/train/yolov73/weights/best.pt'])\n",
            "YOLOR ðŸš€ 2022-10-21 torch 1.12.1+cu113 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 314 layers, 36492560 parameters, 6194944 gradients, 103.2 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "1 uniform, 1 head, 1 body, Done. (11.8ms) Inference, (1.3ms) NMS\n",
            " The image with the result is saved in: runs/detect/exp4/1192.jpg\n",
            "Done. (0.268s)\n"
          ]
        }
      ],
      "source": [
        "!python3 detect.py --weights runs/train/yolov73/weights/best.pt --conf 0.25 --img-size 416 --source ../val/images/1192.jpg"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "bc7c7de4f5879d90a5975f8a44fd1f27b26fdf2d7671f85969080f59463efe95"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
